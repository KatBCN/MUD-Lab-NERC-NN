{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KatBCN/MUD-Lab-NERC-NN/blob/main/experiments/codemap_word_lower_pos_param6/MUDLab6_DDI_word_lower_POS_tagging_params6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change Runtime Type to GPU before running."
      ],
      "metadata": {
        "id": "9c_sPkDRfprM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mX7VXCjWfKeQ"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# mount drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/MUD-Lab-6\n",
        "%ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyhbE3RYqcUV",
        "outputId": "71ff9fe4-b9c1-4e47-c147-03c4ca908819"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/MUD-Lab-6\n",
            "codemaps_lw_pos.py    \u001b[0m\u001b[01;34mdata\u001b[0m/       devel.stats   model.idx\n",
            "codemaps_pos.py       dataset.py  evaluator.py  \u001b[01;34m__pycache__\u001b[0m/\n",
            "codemaps.py           deptree.py  \u001b[01;34mExperiments\u001b[0m/  \u001b[01;34mutil\u001b[0m/\n",
            "codemaps_w_lw_pos.py  devel.out   \u001b[01;34mmodel\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BowuMcDbdnMe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "sys.path.insert(1, \"/content/drive/MyDrive/MUD-Lab-6\")\n",
        "\n",
        "import random\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "from tensorflow.keras import regularizers, Input\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Conv1D, MaxPool1D, Reshape, Concatenate, Flatten, Bidirectional, LSTM\n",
        "\n",
        "\n",
        "from deptree import *\n",
        "from dataset import *\n",
        "# codemaps for lowercase words and pos tags\n",
        "from codemaps_w_lw_pos import *\n",
        "import evaluator\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(23)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment with words, lowercase words, and POS tagging\n",
        "\n",
        "- Uses 40 as the output_dim for all embeddings.\n",
        "-  Conv1D filters to 30\n",
        "- Using 10 epochs\n",
        "- kernel size:3, stride: 2\n",
        "\n",
        "This model uses code provided by the course."
      ],
      "metadata": {
        "id": "pa62EjZFfCo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EVs3b3mChb01"
      },
      "outputs": [],
      "source": [
        "#this is building the neural network.\n",
        "def build_network(idx) :\n",
        "\n",
        "   # sizes\n",
        "   n_words = codes.get_n_words() # number of words in vocabulary\n",
        "   n_lc_words = codes.get_n_lc_words() # number of lowercase words in vocabulary\n",
        "   n_pos = codes.get_n_pos() # number of pos tags\n",
        "   max_len = codes.maxlen\n",
        "   n_labels = codes.get_n_labels()\n",
        "\n",
        "   # word input layer & embeddings\n",
        "   inptW = Input(shape=(max_len,)) # shape of input: all vectors need to be the same length.\n",
        "   embW = Embedding(input_dim=n_words, output_dim=40, # output_dim is the hyperparameter that can be tuned.\n",
        "                      input_length=max_len, mask_zero=False)(inptW)  \n",
        "\n",
        "  # lowercase word input layer & embeddings\n",
        "   inptLC = Input(shape=(max_len,)) # shape of input: all vectors need to be the same length.\n",
        "   embLC = Embedding(input_dim=n_lc_words, output_dim=40, # output_dim is the hyperparameter that can be tuned.\n",
        "                      input_length=max_len, mask_zero=False)(inptLC)\n",
        "\n",
        "  # POS input layer & embeddings\n",
        "   inptPOS = Input(shape=(max_len,)) # shape of input: all vectors need to be the same length.\n",
        "   embPOS = Embedding(input_dim=n_pos, output_dim=40, # output_dim is the hyperparameter that can be tuned.\n",
        "                      input_length=max_len, mask_zero=False)(inptPOS)\n",
        "\n",
        "   conc = Concatenate()([embW, embLC, embPOS])\n",
        "\n",
        "  # number of filters it the output dimension which can be tuned.\n",
        "   conv = Conv1D(filters=30, kernel_size=3, strides=2, activation='relu', padding='same')(conc) # kernel size is the context of words, depending on the stride, you can have overlapping.\n",
        "   flat= Flatten()(conv) # concatenating the vectors one after the other - we can change this architecture.\n",
        "\n",
        "   # we need a single vector\n",
        "   \n",
        "   out = Dense(n_labels, activation='softmax')(flat)\n",
        "\n",
        "   model = Model([inptW, inptLC, inptPOS], out)\n",
        "   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "   return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZuXt4Sf1ifKc"
      },
      "outputs": [],
      "source": [
        "# directory with files to process\n",
        "trainfile = \"/content/drive/MyDrive/MUD-Lab-6/data/train.pck\"\n",
        "validationfile = \"/content/drive/MyDrive/MUD-Lab-6/data/devel.pck\"\n",
        "modelname = \"model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN5elfAkhjqf",
        "outputId": "9b810c16-85bb-453f-9776-279cb960301d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 150, 40)      182120      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 150, 40)      159320      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 150, 40)      2040        ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 150, 120)     0           ['embedding[0][0]',              \n",
            "                                                                  'embedding_1[0][0]',            \n",
            "                                                                  'embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 75, 30)       10830       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 2250)         0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            11255       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 365,565\n",
            "Trainable params: 365,565\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "724/724 [==============================] - 16s 6ms/step - loss: 0.4965 - accuracy: 0.8529 - val_loss: 0.4947 - val_accuracy: 0.8460\n",
            "Epoch 2/10\n",
            "724/724 [==============================] - 4s 5ms/step - loss: 0.2977 - accuracy: 0.8901 - val_loss: 0.4282 - val_accuracy: 0.8696\n",
            "Epoch 3/10\n",
            "724/724 [==============================] - 4s 5ms/step - loss: 0.2194 - accuracy: 0.9177 - val_loss: 0.4234 - val_accuracy: 0.8687\n",
            "Epoch 4/10\n",
            "724/724 [==============================] - 4s 5ms/step - loss: 0.1838 - accuracy: 0.9300 - val_loss: 0.4191 - val_accuracy: 0.8694\n",
            "Epoch 5/10\n",
            "724/724 [==============================] - 4s 5ms/step - loss: 0.1611 - accuracy: 0.9387 - val_loss: 0.4308 - val_accuracy: 0.8713\n",
            "Epoch 6/10\n",
            "724/724 [==============================] - 4s 5ms/step - loss: 0.1434 - accuracy: 0.9455 - val_loss: 0.4541 - val_accuracy: 0.8759\n",
            "Epoch 7/10\n",
            "724/724 [==============================] - 4s 5ms/step - loss: 0.1308 - accuracy: 0.9511 - val_loss: 0.4393 - val_accuracy: 0.8731\n",
            "Epoch 8/10\n",
            "724/724 [==============================] - 4s 5ms/step - loss: 0.1185 - accuracy: 0.9548 - val_loss: 0.4506 - val_accuracy: 0.8692\n",
            "Epoch 9/10\n",
            "724/724 [==============================] - 4s 5ms/step - loss: 0.1112 - accuracy: 0.9579 - val_loss: 0.4630 - val_accuracy: 0.8767\n",
            "Epoch 10/10\n",
            "724/724 [==============================] - 4s 5ms/step - loss: 0.1021 - accuracy: 0.9621 - val_loss: 0.5135 - val_accuracy: 0.8763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ]
        }
      ],
      "source": [
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  train.py ../data/Train ../data/Devel  modelname\n",
        "## --\n",
        "\n",
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  train.py ../data/Train ../data/Devel  modelname\n",
        "## --\n",
        "\n",
        "\n",
        "# directory with files to process\n",
        "# trainfile = sys.argv[1]\n",
        "# validationfile = sys.argv[2]\n",
        "# modelname = sys.argv[3]\n",
        "\n",
        "# load train and validation data\n",
        "traindata = Dataset(trainfile)\n",
        "valdata = Dataset(validationfile)\n",
        "\n",
        "# create indexes from training data\n",
        "max_len = 150\n",
        "suf_len = 5\n",
        "codes = Codemaps(traindata, max_len)\n",
        "\n",
        "# build network\n",
        "model = build_network(codes)\n",
        "with redirect_stdout(sys.stderr) :\n",
        "   model.summary()\n",
        "\n",
        "# encode datasets\n",
        "Xt = codes.encode_words(traindata)\n",
        "Yt = codes.encode_labels(traindata)\n",
        "Xv = codes.encode_words(valdata)\n",
        "Yv = codes.encode_labels(valdata)\n",
        "\n",
        "#track time to train model\n",
        "start_time = time.time()\n",
        "\n",
        "# train model\n",
        "with redirect_stdout(sys.stderr) :\n",
        "   model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv,Yv), verbose=1) # add class_weight here for experiments.\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "# save model and indexs\n",
        "model.save(modelname)\n",
        "codes.save(modelname)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Amount of time to train the model is:\", train_time)"
      ],
      "metadata": {
        "id": "Pe33L-95i08y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021bebfa-405e-4d66-a0ef-47483b719731"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of time to train the model is: 82.67973351478577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fname = \"/content/drive/MyDrive/MUD-Lab-6/model\"\n",
        "datafile = \"/content/drive/MyDrive/MUD-Lab-6/data/devel.pck\"\n",
        "outfile = \"devel.out\""
      ],
      "metadata": {
        "id": "jq2RfnjNpFmc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code chunk is from predict.py"
      ],
      "metadata": {
        "id": "SXlLXmYxoR7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict.py\n",
        "\n",
        "## --------- Entity extractor ----------- \n",
        "## -- Extract drug entities from given text and return them as\n",
        "## -- a list of dictionaries with keys \"offset\", \"text\", and \"type\"\n",
        "\n",
        "def output_interactions(data, preds, outfile) :\n",
        "\n",
        "   #print(testdata[0])\n",
        "   outf = open(outfile, 'w')\n",
        "   for exmp,tag in zip(data.sentences(),preds) :\n",
        "      sid = exmp['sid']\n",
        "      e1 = exmp['e1']\n",
        "      e2 = exmp['e2']\n",
        "      if tag!='null' :\n",
        "         print(sid, e1, e2, tag, sep=\"|\", file=outf)\n",
        "            \n",
        "   outf.close()\n",
        "\n",
        "   \n",
        "## --------- MAIN PROGRAM ----------- \n",
        "## --\n",
        "## -- Usage:  baseline-NER.py target-dir\n",
        "## --\n",
        "## -- Extracts Drug NE from all XML files in target-dir\n",
        "## --\n",
        "\n",
        "#fname = sys.argv[1]\n",
        "#datafile = sys.argv[2]\n",
        "#outfile = sys.argv[3]\n",
        "\n",
        "model = load_model(fname)\n",
        "codes = Codemaps(fname)\n",
        "\n",
        "testdata = Dataset(datafile)\n",
        "X = codes.encode_words(testdata)\n",
        "\n",
        "Y = model.predict(X)\n",
        "Y = [codes.idx2label(np.argmax(s)) for s in Y]\n",
        "\n",
        "# extract relations\n",
        "output_interactions(testdata, Y, outfile)\n"
      ],
      "metadata": {
        "id": "vHXEGZNkn3Rt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"DDI\"\n",
        "golddir = \"/content/drive/MyDrive/MUD-Lab-6/data/devel\"\n",
        "outfile = \"devel.out\""
      ],
      "metadata": {
        "id": "Vb9e-qHwrpaC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code chunk is from evaluator.py and creates file devel.stats"
      ],
      "metadata": {
        "id": "JOIWKxLpoisY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluator.py\n",
        "\n",
        "#! /usr/bin/python3\n",
        "\n",
        "import sys\n",
        "from os import listdir\n",
        "\n",
        "from xml.dom.minidom import parse\n",
        "\n",
        "## --\n",
        "## -- auxliary to insert an instance in given instance_set\n",
        "## --\n",
        "\n",
        "def add_instance(instance_set, einfo, etype) :\n",
        "    instance_set[\"CLASS\"].add(einfo+\"|\"+etype)\n",
        "    instance_set[\"NOCLASS\"].add(einfo)\n",
        "    if etype not in instance_set : instance_set[etype] = set([])\n",
        "    instance_set[etype].add(einfo)\n",
        "\n",
        "    \n",
        "## --\n",
        "## -- Load entities from XML files in given golddir\n",
        "## --\n",
        "\n",
        "\"\"\"\n",
        "def load_gold_NER(golddir) :\n",
        "    entities = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "\n",
        "    # process each file in directory\n",
        "    for f in listdir(golddir) :\n",
        "\n",
        "        # parse XML file, obtaining a DOM tree\n",
        "        tree = parse(golddir+\"/\"+f)\n",
        "\n",
        "        # process each sentence in the file\n",
        "        sentences = tree.getElementsByTagName(\"sentence\")\n",
        "        for s in sentences :\n",
        "            sid = s.attributes[\"id\"].value   # get sentence id\n",
        "            \n",
        "            # load sentence entities\n",
        "            ents = s.getElementsByTagName(\"entity\")\n",
        "            for e in ents :\n",
        "                einfo = sid + \"|\" + e.attributes[\"charOffset\"].value  + \"|\" + e.attributes[\"text\"].value\n",
        "                etype = e.attributes[\"type\"].value\n",
        "                add_instance(entities, einfo, etype)\n",
        "            \n",
        "    return entities\n",
        "\"\"\"\n",
        "## --\n",
        "## -- Load relations from XML files in given golddir\n",
        "## --\n",
        "\n",
        "def load_gold_DDI(golddir) :\n",
        "    relations = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "\n",
        "    # process each file in directory\n",
        "    for f in listdir(golddir) :\n",
        "\n",
        "        # parse XML file, obtaining a DOM tree\n",
        "        tree = parse(golddir+\"/\"+f)\n",
        "\n",
        "        # process each sentence in the file\n",
        "        sentences = tree.getElementsByTagName(\"sentence\")\n",
        "        for s in sentences :\n",
        "            sid = s.attributes[\"id\"].value   # get sentence id\n",
        "        \n",
        "            # load \"pairs\"  in the sentence, keep those with ddi=true\n",
        "            pairs = s.getElementsByTagName(\"pair\")\n",
        "            for p in pairs:\n",
        "                id_e1 = p.attributes[\"e1\"].value\n",
        "                id_e2 = p.attributes[\"e2\"].value\n",
        "                ddi = p.attributes[\"ddi\"].value\n",
        "\n",
        "                if (ddi == \"true\") :\n",
        "                    rtype = p.attributes[\"type\"].value\n",
        "                    rinfo = sid + \"|\" + id_e1 + \"|\" +  id_e2\n",
        "                    add_instance(relations, rinfo, rtype)\n",
        "\n",
        "    return relations\n",
        "\n",
        "\n",
        "## --\n",
        "## -- Load entities/relations from given system output file\n",
        "## --\n",
        "\n",
        "def load_predicted(task, outfile) :\n",
        "    predicted = { \"CLASS\" : set([]), \"NOCLASS\" : set([]) }\n",
        "    outf = open(outfile,\"r\")\n",
        "    for line in outf.readlines() :\n",
        "        line = line.strip()\n",
        "        if line in predicted[\"CLASS\"] :\n",
        "            print(\"Ignoring duplicated entity in system predictions file: \"+line)\n",
        "            continue\n",
        "\n",
        "        etype = line.split(\"|\")[-1]\n",
        "        einfo = \"|\".join(line.split(\"|\")[:-1])\n",
        "        add_instance(predicted, einfo, etype)\n",
        "        outf.close()\n",
        "        \n",
        "    return predicted\n",
        "    \n",
        "\n",
        "\n",
        "## --\n",
        "## -- Compare given sets and compute tp,fp,fn,P,R,F1\n",
        "## --\n",
        "\n",
        "def statistics(gold,predicted,kind) :\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    nexp = len(gold[kind])\n",
        "    if kind in predicted:\n",
        "        npred = len(predicted[kind])\n",
        "        for p in predicted[kind] :\n",
        "            if p in gold[kind] : tp += 1\n",
        "            else : fp += 1\n",
        "\n",
        "        fn = 0\n",
        "        for p in gold[kind] :\n",
        "            if p not in predicted[kind] : fn += 1\n",
        "\n",
        "    else :\n",
        "        npred = 0\n",
        "        fn = nexp\n",
        "\n",
        "    P = tp/npred if npred!=0 else 0\n",
        "    R = tp/nexp if nexp!=0 else 0    \n",
        "    F1 = 2*P*R/(P+R) if P+R!=0 else 0\n",
        "\n",
        "    return tp,fp,fn,npred,nexp,P,R,F1\n",
        "\n",
        "## --\n",
        "## -- Compute and print statistics table\n",
        "## --\n",
        "\n",
        "def row(txt) :\n",
        "   return txt + ' '*(17-len(txt))\n",
        "\n",
        "\n",
        "def print_statistics(gold,predicted, statfile) :\n",
        "    print(row(\"\")+\"  tp\\t  fp\\t  fn\\t#pred\\t#exp\\tP\\tR\\tF1\", file=statfile)\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    (nk,sP,sR,sF1) = (0,0,0,0)\n",
        "    for kind in sorted(gold) :\n",
        "        if kind==\"CLASS\" or kind==\"NOCLASS\" : continue\n",
        "        (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, kind)\n",
        "        print(row(kind)+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)\n",
        "        (nk,sP,sR,sF1) = (nk+1, sP+P, sR+R, sF1+F1)\n",
        "\n",
        "    (sP, sR, sF1) = (sP/nk, sR/nk, sF1/nk)\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    print(row(\"M.avg\")+\"-\\t-\\t-\\t-\\t-\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(sP, sR, sF1), file=statfile)\n",
        "\n",
        "    print(\"------------------------------------------------------------------------------\", file=statfile)\n",
        "    (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, \"CLASS\")\n",
        "    print(row(\"m.avg\")+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)                        \n",
        "    (tp,fp,fn,npred,nexp,P,R,F1) = statistics(gold, predicted, \"NOCLASS\")\n",
        "    print(row(\"m.avg(no class)\")+\"{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:>4}\\t{:2.1%}\\t{:2.1%}\\t{:2.1%}\".format(tp,fp,fn,npred,nexp, P, R, F1), file=statfile)               \n",
        "\n",
        "## --\n",
        "## -- Evaluates results in outfile comparing them with gold standard in golddir.\n",
        "## -- 'task' is either NER or DDI\n",
        "## -- This function can be called from any program requesting evaluation.\n",
        "## --\n",
        " \n",
        "def evaluate(task, golddir, outfile):\n",
        "\n",
        "    if task==\"NER\" :\n",
        "        # get set of expected entities in the whole golddir\n",
        "        gold = load_gold_NER(golddir)\n",
        "    elif task == \"DDI\" :\n",
        "        # get set of expected relations in the whole golddir\n",
        "        gold = load_gold_DDI(golddir)\n",
        "    else :\n",
        "        print (\"Invalid task '\"+task+\"'. Please specify 'NER' or 'DDI'.\")        \n",
        "\n",
        "\n",
        "    # Load entities/relations predicted by the system\n",
        "    predicted = load_predicted(task, outfile)\n",
        "\n",
        "    # compare both sets and compute statistics\n",
        "    statfile = open(\"devel.stats\", 'w')\n",
        "    print_statistics(gold,predicted, statfile)\n",
        "    statfile.close()\n",
        "\n",
        "         \n",
        "        \n",
        "## --\n",
        "## -- Usage as standalone program:  evaluator.py (NER|DDI) golddir outfile\n",
        "## --\n",
        "## -- Evaluates results in outfile comparing them with gold standard in golddir\n",
        "## --\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #if len(sys.argv) != 4 :\n",
        "     #   print(\"\\n  Usage: evaluator.py (NER|DDI) golddir outfile\\n\")\n",
        "      #  exit()\n",
        "        \n",
        "    task = \"DDI\"\n",
        "    golddir = \"/content/drive/MyDrive/MUD-Lab-6/data/devel\"\n",
        "    outfile = \"devel.out\"\n",
        "\n",
        "    evaluate(task, golddir, outfile)\n"
      ],
      "metadata": {
        "id": "hOK3PizbohBR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results : Experiment with Different Parameters\n",
        "\n",
        "devel.stats:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
        "------------------------------------------------------------------------------\n",
        "advise             87\t  70\t  54\t 157\t 141\t55.4%\t61.7%\t58.4%\n",
        "effect            143\t  66\t 169\t 209\t 312\t68.4%\t45.8%\t54.9%\n",
        "int                14\t   7\t  14\t  21\t  28\t66.7%\t50.0%\t57.1%\n",
        "mechanism          58\t  29\t 203\t  87\t 261\t66.7%\t22.2%\t33.3%\n",
        "------------------------------------------------------------------------------\n",
        "M.avg            -\t-\t-\t-\t-\t64.3%\t44.9%\t50.9%\n",
        "------------------------------------------------------------------------------\n",
        "m.avg             302\t 172\t 440\t 474\t 742\t63.7%\t40.7%\t49.7%\n",
        "m.avg(no class)   343\t 131\t 399\t 474\t 742\t72.4%\t46.2%\t56.4%\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "iV7VKi24usa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison Experiment Results using Word, Lowercase word, and POS embeddings.\n",
        "\n",
        "This experiment increased the M.avg F1 score over the baseline slightly."
      ],
      "metadata": {
        "id": "7Q_NL_Y3pkbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "devel.stats:\n",
        "\n",
        "```\n",
        "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
        "------------------------------------------------------------------------------\n",
        "advise             93\t  53\t  48\t 146\t 141\t63.7%\t66.0%\t64.8%\n",
        "effect            156\t  98\t 156\t 254\t 312\t61.4%\t50.0%\t55.1%\n",
        "int                16\t   7\t  12\t  23\t  28\t69.6%\t57.1%\t62.7%\n",
        "mechanism          73\t  59\t 188\t 132\t 261\t55.3%\t28.0%\t37.2%\n",
        "------------------------------------------------------------------------------\n",
        "M.avg            -\t-\t-\t-\t-\t62.5%\t50.3%\t55.0%\n",
        "------------------------------------------------------------------------------\n",
        "m.avg             338\t 217\t 404\t 555\t 742\t60.9%\t45.6%\t52.1%\n",
        "m.avg(no class)   373\t 182\t 369\t 555\t 742\t67.2%\t50.3%\t57.5%\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2-_Km0JGnkS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison Baseline Results:"
      ],
      "metadata": {
        "id": "paHfdeP9Jac1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "devel.stats:\n",
        "\n",
        "```\n",
        "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
        "------------------------------------------------------------------------------\n",
        "advise             90\t  54\t  51\t 144\t 141\t62.5%\t63.8%\t63.2%\n",
        "effect            160\t  98\t 152\t 258\t 312\t62.0%\t51.3%\t56.1%\n",
        "int                14\t   5\t  14\t  19\t  28\t73.7%\t50.0%\t59.6%\n",
        "mechanism          64\t  53\t 197\t 117\t 261\t54.7%\t24.5%\t33.9%\n",
        "------------------------------------------------------------------------------\n",
        "M.avg            -\t-\t-\t-\t-\t63.2%\t47.4%\t53.2%\n",
        "------------------------------------------------------------------------------\n",
        "m.avg             328\t 210\t 414\t 538\t 742\t61.0%\t44.2%\t51.2%\n",
        "m.avg(no class)   359\t 179\t 383\t 538\t 742\t66.7%\t48.4%\t56.1%\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "v9IJjNoBI--j"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "MUDLab6-DDI_word_lower_POS tagging_params6.ipynb",
      "provenance": [],
      "mount_file_id": "1h5U2v8X1ObVeIljJMEpfPVkUVi3L7h8G",
      "authorship_tag": "ABX9TyMrYwV8OvLAesn4/IXTi+vn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}